---
title: "practical machine learning Project"
author: "Balogun Stephen Taiye"
date: "7th May, 2016 "
output: 
  html_document: 
    fig_caption: yes
    fig_height: 3
    fig_width: 5
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, verbose = FALSE, collapse = TRUE, error = FALSE, message = FALSE)
```

# Introduction

This is the project report for the practical machine learning course in the Data Science Specialization offered by the John Hopkins Bloomberg School of Medicine.  The course focuses of using algorithm to train `R` to be able to predict the outcome of a given data entry using data with known outcomes.

# The Project Goal  

This project consists of two given datasets viz: the **training** and the **testing** datasets. The goal of the project is to:  

  #. use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants in the **training** dataset to develop a prediction algorithm for the manner in which the exercise was performed. The manner of exercise performance is represented by the "classe" variable in the **training** dataset. 
  #. create a report describing how:     
      * you built your model  
      *  how you used cross validation  
      *  what you think the expected out of sample error is, and why you made the choices you did.  
  #. use your prediction model to predict 20 different test cases in the *testing*  dataset was performed.   
  
  I am grateful to <http://groupware.les.inf.puc-rio.br/har> for providing the data for this project.  

  
#  Building the prediction algorithm

##  Downloading and reading the datasets into R

I downloaded the data from the source using the `downloader` package  
```{r, "download the training and the testing datasets"}
library("downloader")   ## loads the downloader package for downloading the datasets
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if(!file.exists("pml-training.csv")) {download(trainUrl, 
                                               destfile = "./pml-training.csv")}
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("pml-testing.csv")) {download(testUrl, 
                                              destfile = "./pml-testing.csv")}
```

Having opened the `*.csv` files, i realised that missing data are represented by `NA` and blank spaces so I read-in the files using `na.strings` as `c(("NA", "")`  
```{r, "readData"}
train <- read.csv("./pml-training.csv", na.strings = c("NA", ""))
testdata <- read.csv("./pml-testing.csv", na.strings = c("NA", ""))

### load the required packages

library("caret")     ## for the algorithm and most of the works of this project
library("printr")   ## displays nicely formattted tables
```


## Exploratory and cleaning data
```{r, "exploratory data analysis"}
dim(train)   ## checks for the number of rows and columns of the train data
dim(train[complete.cases(train), ])  ## checks for the number of rows with no missing data
length(is.na(train))   ## checks for the number of times there are missing entries
```

Considerin the large number of missing data, I decided to build a function that selects only columns with complete entries from the 160 columns of the datasets to use for my prediction algorithm 
```{r, "function select columns with complete entries"}
colnames_train <- colnames(train)
nonNa <- function(x) {
as.vector(apply(x, 2, function(x)
length(which(!is.na(x)))))
}

colcnts <- nonNa(train)
drops <- c()
for (cnt in 1:length(colcnts)) {
if (colcnts[cnt] < nrow(train)) {
drops <- c(drops, 
colnames_train[cnt])
}
}
```

Next i apply the function on both the training and the testing datasets

```{r, "dropping columns with missing data"}
trainin <- train[, !(names(train) %in% drops)] ## drops columns with missing entries for the training dataset
colnames_testdata <- colnames(testdata)
testdata <- testdata[, !(names(testdata) %in% drops)] ## drops columns with missing entries for the testing dataset 
```

Having gone through the dataset, i realised the first 7 columns are only id variables and are not necessary for analysis so I will be dropping them now.

```{r, "dropping first 7 columns"}
trainin <- trainin[, 7:length(colnames(trainin))]  ## for the training dataset
testdata <- testdata[, 7:length(colnames(testdata))]   ## for the testing dataset
```

##  Training the algorithm
Considering the huge size of the training dataset, I decided to split the dataset into 3, use the first partition to *train* the algorithm and the second for *testing* my algorithm and refining the algorithm as necessary, then the third for *validating* my algorithm before using it to predict the **test** dataset.  


```{r, partition}
set.seed(12345)
inTrain1 <- createDataPartition(y = train$classe, p = 0.4, list = FALSE)
traininset <- trainin[inTrain1, ]  ## creates a dataset from training the algorithm
testValid <- trainin[-inTrain1, ]
inTrain2 <- createDataPartition(y = testValid$classe, p = .5, list = FALSE)
test <- testValid[inTrain2, ]   ## creates the dataset for testing the algorithm
valid <- testValid[-inTrain2, ]  ## creates the dataset for validating the algorithm 
```

Next I pre-process my training dataset checking for nearzero variables, and very high correlation (correlation greater than 0.99)

```{r, preprocessing}
nzv <- nearZeroVar(traininset)
nzv   ## no nearZero variables in the dataset

##  next i check for very high correlation and eliminate variables with correlations > 0.99.
cor <- abs(cor(x = traininset[,-54]))  
diag(cor) <- 0  ## sets the diagonals of the correlation table to zero (i.e. sets the correlation of the variable to itself to zero which ordinarily should be one)
which(cor > 0.99, arr.ind = TRUE)  ## displays correlations greater than 0.99
plot(traininset$accel_belt_z, traininset$roll_belt)  ## plots the correlated variables
```

This shows that columns 2 and 11 are highly correlated and both of these variables are not necessary for my algorithm. So i decided to remove column 2 in my datasets

```{r, "removing columns 2"}
traininset <- traininset[, -2]
test <- test[, -2]
valid <- valid[, -2]
testdata <- testdata[, -2]
```

Having pre-processed my datasets a bit, i built my algorithm using two different models (randomForest and gbm) and cross-check their accuracy. This final models were developed after I have refined my initial models several times using the *traininset* and the *test* data of the original training dataset.

```{r, algorithm, cache = TRUE}
trc <- trainControl(method = "repeatedcv")   ## uses repeated cross-validation for trainControl
modelfit1 <- train(classe ~ ., data = traininset, method = "rf", trControl = trc)
modelfit2 <- train(classe ~ ., data = traininset, method = "gbm", verbose = FALSE)
```
```{r, predicting the "traininset"}
pred1 <- predict(modelfit1, newdata = traininset)    ## predicts the traininset using model1
pred2 <- predict(modelfit2, newdata = traininset)  ## predicts the traininset using model2
confusionMatrix(pred1, traininset$classe)[[3]][1:4]  ## checks the accuracy of the first model
confusionMatrix(pred2, traininset$classe)[[3]][1:4]  ## checks the accuracy of the second model
confusionMatrix(pred1, pred2)[[3]][1:4]  ## checks the agreement between the two models
```

Next I applied the two prediction models on the *test* subset of the training dataset and displayed the summary table of the outcome. The code for this table is available in the `Rmd` file with this submission.
```{r, test, echo = FALSE}
predtest1 <- predict(modelfit1, newdata = test)  ## predicts the test subset of the training dataset
predtest2 <- predict(modelfit2, newdata = test)  ## predicts the test subset of the training daataset
model1 <- confusionMatrix(predtest1, test$classe)[[3]][1:4]  ## checks for the accuracy of the first model in predicting the test subset
model2 <- confusionMatrix(predtest2, test$classe)[[3]][1:4] ## checks the accurary of the second model in predicting the test subset
modelagree <- confusionMatrix(predtest1, predtest2)[[3]][1:4]  ## checks the agreement of both models
p1 <- rbind(model1, model2, modelagree)
p1
```

##  Validating the algorithm
Considering the high accuracy of the two models, I decided to use the two models for validation. The table below shows the accuracy of the algorithms (models 1 and 2), and their agreement on the valid data.
 
```{r, "validation", echo = FALSE}
predvalid1 <- predict(modelfit1, newdata = valid)
predvalid2 <- predict(modelfit2, newdata = valid)
model1 <- confusionMatrix(predvalid1, valid$classe)[[3]][1:4]  ## displays the accuracy of model1
model2 <- confusionMatrix(predvalid2, valid$classe)[[3]][1:4]  ## displays the accuracy of model2
modelagree <- confusionMatrix(predvalid1, predvalid2)[[3]][1:4] ## checks the agreement of both models
p2 <- rbind(model1, model2, modelagree)  ## displays the accuracy of model1, model 2 and the agreement between the two models for the "valid" data
p2  ##  displays the data
```


## calculating the in-sample and out of sample errors

The errors were calculated using the traininset for the in-sample error and the validation set for the out of sample error. I have displayed the result for both model 1 and model 2. The out of sample error is a little more than that of the in-sample error as displayed below but it is still relatively very small.

```{r, "sample errors", echo = FALSE}

## in-sample errors
model1 <- (1 - confusionMatrix(pred1, traininset$classe)[[3]][[1]])  ## for model1
model2 <- (1 - confusionMatrix(pred2, traininset$classe)[[3]][[1]])  ## for model2
a <- data.frame(model1, model2)
rownames(a) <- ("in-sample error")
## out of sample errors
model1 <- (1 - confusionMatrix(predvalid1, valid$classe)[[3]][[1]])
model2 <- (1 - confusionMatrix(predvalid2, valid$classe)[[3]][[1]])
b <- data.frame(model1, model2)
rownames(b) <- ("out of sample error")
rbind(a, b)
```

## Predicting the test dataset

I decided to use the two models to predict the test dataset and compare the accuracy of the predictions to increase the reliability of the prediction.

```{r, "predicting the test data"}
model1test <- predict(modelfit1, newdata = testdata)
model2test <- predict(modelfit2, newdata = testdata)
confusionMatrix(model1test, model2test)
model1test
```


#  Conclusion

The two algorithm have 100% agreement on the **test** dataset. Therefore, I believe the prediction is reliable and has very high prediction accuracy. For a complete review of the code for this project, see the `Rmd` file attached to this submission.


